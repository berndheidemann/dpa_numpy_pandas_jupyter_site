# Pandas ‚Äì Fallstudie Shark Attacks

## Lernziele

Nach Bearbeitung dieses Arbeitsblatts kannst du:

- einen komplexen, realen Datensatz selbstst√§ndig analysieren
- Datenbereinigung bei unstrukturierten Daten durchf√ºhren
- fortgeschrittene Pandas-Techniken anwenden
- aussagekr√§ftige Erkenntnisse aus Daten ableiten

!!! note "Begleitende Infobl√§tter"
    - [:material-book-open-variant: Pandas Grundlagen](../infoblaetter/pandas-grundlagen.md)
    - [:material-book-open-variant: Pandas Aggregation](../infoblaetter/pandas-aggregation.md)
    - [:material-book-open-variant: Datenbereinigung](../infoblaetter/datenbereinigung.md)

---

## Einf√ºhrung

Der Global Shark Attack File (GSAF) ist eine Datenbank aller dokumentierten Haiangriffe weltweit. Der Datensatz enth√§lt viele fehlende Werte und unstrukturierte Textdaten ‚Äì eine realistische Herausforderung!

!!! abstract "Datensatz herunterladen"
    [:material-download: global_shark_attacks.csv](../assets/files/global_shark_attacks.csv){ .md-button }

<div class="notebook-hint" markdown>
<span class="notebook-hint-icon">üìì</span>
<div class="notebook-hint-text">
<strong>Bearbeite alle Aufgaben in einem Jupyter Notebook</strong>
<small>√ñffne JupyterLite direkt im Browser ‚Äì keine Installation n√∂tig!</small>
</div>
<a href="https://berndheidemann.github.io/dpa_numpy_pandas_jupyter_site/jupyter/lab/index.html" target="_blank" class="jupyter-button">
üöÄ JupyterLite √∂ffnen
</a>
</div>

---

## Aufgaben

### Aufgabe 1 ‚Äì Daten laden und ersten √úberblick gewinnen

Lade den Shark-Attacks-Datensatz und verschaffe dir einen √úberblick.

- [ ] Lade den Datensatz `global_shark_attacks.csv` ‚Äì beachte das Encoding f√ºr Sonderzeichen
- [ ] Gib Shape und Spaltennamen aus
- [ ] Zeige die ersten Zeilen an (transponiert f√ºr bessere Lesbarkeit)
- [ ] Analysiere Datentypen und fehlende Werte ‚Äì wie viele NaN-Werte hat jede Spalte?

!!! tip "Hilfe"
    - Laden mit Encoding: `pd.read_csv(pfad, encoding='latin-1')`
    - Transponierte Ansicht: `df.head().T`
    - Fehlende Werte: `df.isnull().sum().sort_values(ascending=False)`
    - √úberblick: `df.info()`

!!! question "Reflexionsfrage"
    Welche Spalten haben besonders viele fehlende Werte? Woran k√∂nnte das liegen?

---

### Aufgabe 2 ‚Äì Relevante Spalten ausw√§hlen

Der Datensatz hat viele Spalten. W√§hle die wichtigsten f√ºr die Analyse aus.

- [ ] Liste alle Spaltennamen nummeriert auf
- [ ] Identifiziere relevante Spalten wie: Year, Country, Area, Location, Activity, Name, Sex, Age, Injury, Fatal (Y/N), Time, Species
- [ ] Erstelle einen Arbeits-DataFrame, der nur diese Spalten enth√§lt (verwende `.copy()`)
- [ ] Pr√ºfe, welche Spalten tats√§chlich im Datensatz vorhanden sind

!!! tip "Hilfe"
    - Spalten auflisten: `df.columns.tolist()` oder mit enumerate durchlaufen
    - Spalten filtern: `df[liste_der_spalten].copy()`
    - Spalte pr√ºfen: `'Spaltenname' in df.columns`

---

### Aufgabe 3 ‚Äì Daten bereinigen

Reale Daten sind "messy" ‚Äì standardisiere und bereinige die wichtigsten Spalten.

**Jahr bereinigen:**

- [ ] Pr√ºfe den Datentyp der Jahr-Spalte
- [ ] Konvertiere zu numerisch ‚Äì ung√ºltige Werte sollen NaN werden
- [ ] Entferne unrealistische Jahre (vor 1800 oder in der Zukunft)
- [ ] Gib den g√ºltigen Zeitraum und die Anzahl nach Filterung aus

!!! tip "Hilfe"
    - Numerisch konvertieren: `pd.to_numeric(df['Year'], errors='coerce')`
    - Aktuelles Jahr: `datetime.datetime.now().year`
    - Filter: `df[(df['Year'] >= 1800) & (df['Year'] <= aktuelles_jahr)]`

**Fatal (t√∂dlich) bereinigen:**

- [ ] Zeige die vorhandenen Werte in der Fatal-Spalte mit `value_counts(dropna=False)`
- [ ] Standardisiere auf Gro√übuchstaben und entferne Leerzeichen
- [ ] Mappe 'Y' ‚Üí True und 'N' ‚Üí False
- [ ] Pr√ºfe das Ergebnis

!!! tip "Hilfe"
    - Standardisieren: `df['Spalte'].str.upper().str.strip()`
    - Mapping: `df['Spalte'].map({'Y': True, 'N': False})`

**Alter bereinigen:**

- [ ] Konvertiere das Alter zu numerisch
- [ ] Setze unrealistische Werte (< 0 oder > 100) auf NaN
- [ ] Berechne deskriptive Statistiken f√ºr das bereinigte Alter

!!! tip "Hilfe"
    - Bedingte Zuweisung: `df.loc[bedingung, 'Spalte'] = np.nan`

**Geschlecht standardisieren:**

- [ ] Zeige die vorhandenen Geschlechts-Werte
- [ ] Standardisiere auf 'Male' und 'Female'
- [ ] Pr√ºfe das Ergebnis

---

### Aufgabe 4 ‚Äì Deskriptive Statistik

Berechne grundlegende Statistiken zum Datensatz.

- [ ] Ermittle: Gesamtanzahl Angriffe, Zeitraum (fr√ºhestes und sp√§testes Jahr), Anzahl verschiedener L√§nder
- [ ] Erstelle eine Top-10-Liste der L√§nder nach Anzahl der Angriffe
- [ ] Berechne, welchen Anteil die Top-10-L√§nder am Gesamtdatensatz haben
- [ ] Analysiere die h√§ufigsten Aktivit√§ten (Top 10)
- [ ] Erstelle Altersgruppen (0-10, 11-20, 21-30, usw.) und z√§hle die Angriffe pro Gruppe

!!! tip "Hilfe"
    - Eindeutige Werte: `df['Spalte'].nunique()`
    - H√§ufigkeiten: `df['Spalte'].value_counts().head(10)`
    - Kategorien erstellen: `pd.cut(df['Age'], bins=[0, 10, 20, 30, 40, 50, 60, 100], labels=[...])`

---

### Aufgabe 5 ‚Äì Zeitliche Trends

Analysiere, wie sich Haiangriffe √ºber die Zeit entwickelt haben.

- [ ] Z√§hle Angriffe pro Jahr mit `groupby`
- [ ] Zeige die letzten 10 Jahre
- [ ] Berechne den Durchschnitt, das Maximum und Minimum f√ºr die Jahre ab 2000 ‚Äì in welchem Jahr gab es die meisten Angriffe?
- [ ] Erstelle eine Dekaden-Spalte (1900, 1910, 1920, ...) 
- [ ] Berechne pro Dekade: Anzahl Angriffe und T√∂dlichkeitsrate in Prozent

!!! tip "Hilfe"
    - Gruppieren: `df.groupby('Year').size()`
    - Dekade berechnen: `(df['Year'] // 10) * 10`
    - Index des Maximums: `series.idxmax()`
    - Lambda f√ºr Prozentwerte: `lambda x: x.mean() * 100`

!!! question "Reflexionsfrage"
    Steigt die Anzahl der Angriffe √ºber die Jahrzehnte? Bedeutet das, dass Haie gef√§hrlicher werden, oder gibt es andere Erkl√§rungen?

---

### Aufgabe 6 ‚Äì T√∂dliche Angriffe analysieren

Untersuche die T√∂dlichkeit von Haiangriffen genauer.

- [ ] Berechne die Gesamt-T√∂dlichkeitsrate
- [ ] Berechne die T√∂dlichkeitsrate pro Dekade (ab 1950) ‚Äì gibt es einen Trend?
- [ ] Erstelle eine L√§nder-Statistik (mind. 50 Angriffe): Anzahl Angriffe, Anzahl t√∂dlich, T√∂dlichkeitsrate
- [ ] Sortiere nach Anzahl Angriffe und zeige die Top 10
- [ ] Erstelle eine Aktivit√§ts-Statistik (mind. 20 F√§lle): Welche Aktivit√§ten sind am gef√§hrlichsten?

!!! tip "Hilfe"
    - Rate bei boolean: `df['Fatal'].mean() * 100` (mean auf True/False gibt Anteil True)
    - Mehrere Aggregationen: `df.groupby('Spalte').agg(Name1=('Spalte1', 'count'), Name2=('Spalte2', 'mean'))`
    - Filtern nach Mindestanzahl: `stats[stats['Anzahl'] >= 50]`

---

## Vertiefende Aufgaben

!!! tip "Optionale Aufgaben zur Vertiefung"
    Die folgenden Aufgaben sind **optional** und vertiefen das Gelernte. Sie eignen sich besonders f√ºr:
    
    - **Tiefere Analysen nach Geschlecht, Alter und L√§ndern**
    - **Komplexe Pivot-Tabellen erstellen**
    - **Pr√ºfungsvorbereitung** durch eigenst√§ndiges Arbeiten

---

### Aufgabe 7 ‚Äì Tiefere Analysen

F√ºhre detailliertere Untersuchungen durch.

**Geschlechtervergleich:**

- [ ] Gruppiere nach Geschlecht und berechne: Anzahl, Durchschnittsalter, T√∂dlichkeitsrate
- [ ] Berechne den prozentualen Anteil jedes Geschlechts

**Altersanalyse:**

- [ ] Berechne Anzahl und T√∂dlichkeitsrate pro Altersgruppe
- [ ] Vergleiche das Durchschnittsalter bei t√∂dlichen vs. nicht-t√∂dlichen Angriffen

**L√§nderprofile:**

- [ ] Erstelle f√ºr die Top-5-L√§nder jeweils ein Profil mit:
    - Anzahl Angriffe
    - Zeitraum (fr√ºhester bis sp√§tester Angriff)
    - T√∂dlichkeitsrate
    - H√§ufigste Aktivit√§t (Modus)
    - Durchschnittsalter

!!! tip "Hilfe"
    - Modus (h√§ufigster Wert): `df['Spalte'].mode().iloc[0]`
    - Subset erstellen: `df[df['Country'] == land]`

---

### Aufgabe 8 ‚Äì Pivot-Tabellen erstellen

Nutze Pivot-Tabellen f√ºr komplexere Kreuztabellen.

- [ ] Erstelle eine Pivot-Tabelle: Zeilen = Top-5-L√§nder, Spalten = Dekaden (ab 1950), Werte = Anzahl Angriffe
- [ ] Erstelle eine Pivot-Tabelle: Zeilen = Top-5-L√§nder, Spalten = Top-5-Aktivit√§ten, Werte = T√∂dlichkeitsrate in Prozent

!!! tip "Hilfe"
    - Pivot-Tabelle: `pd.pivot_table(df, values='Spalte', index='Zeilen', columns='Spalten', aggfunc='count', fill_value=0)`
    - F√ºr Prozentwerte: `aggfunc='mean'` und dann `* 100`

---

### Aufgabe 9 ‚Äì Erkenntnisse dokumentieren

Erstelle eine professionelle Zusammenfassung deiner Analyse.

- [ ] Erstelle einen formatierten Ergebnis-Report mit folgenden Abschnitten:
    - **Datensatz**: Anzahl Angriffe, Zeitraum, Anzahl L√§nder
    - **Risiko**: Gesamt-T√∂dlichkeitsrate, gef√§hrlichstes Land, sicherste Aktivit√§t
    - **Demografie**: Durchschnittsalter, Geschlechterverteilung
    - **Trends**: Vergleich der Angriffszahlen zwischen Dekaden
- [ ] Nutze f-Strings f√ºr formatierte Ausgaben mit Tausendertrennzeichen und Nachkommastellen

!!! tip "Hilfe"
    - Tausendertrennzeichen: `f"{zahl:,}"`
    - Eine Nachkommastelle: `f"{wert:.1f}"`
    - Prozent: `f"{rate:.1f}%"`

---

## Bonus-Aufgaben

!!! warning "Ohne Hilfe l√∂sen"
    Bearbeite diese Erweiterungen selbstst√§ndig.

**A) Hai-Arten analysieren:**

- Extrahiere Hai-Arten aus der Species-Spalte (Textverarbeitung n√∂tig)
- Welche Hai-Art ist am h√§ufigsten dokumentiert?
- Welche Art hat die h√∂chste T√∂dlichkeitsrate?

**B) Text Mining auf Verletzungen:**

- Analysiere die Injury-Beschreibungen
- Welche K√∂rperteile werden am h√§ufigsten verletzt?
- Nutze String-Methoden wie `str.contains()` um nach Schl√ºsselw√∂rtern zu suchen

**C) Geographische Hotspots:**

- Gruppiere nach Area/Location innerhalb der Top-L√§nder
- Gibt es regionale Hotspots?
- Welche Regionen in den USA/Australien sind besonders betroffen?

**D) Korrelationsanalyse:**

- Gibt es einen Zusammenhang zwischen Alter und T√∂dlichkeit?
- Unterscheidet sich die T√∂dlichkeitsrate nach Geschlecht signifikant?
- Analysiere, ob bestimmte Aktivit√§ten bei bestimmten Altersgruppen h√§ufiger vorkommen

---

## Zusammenfassung

!!! success "Das hast du gelernt"
    - **Reale Daten** sind messy ‚Äì Bereinigung ist essentiell
    - **Encoding-Probleme** mit `encoding='latin-1'` l√∂sen
    - **Typenkonvertierung** mit `pd.to_numeric(errors='coerce')`
    - **Aggregation** mit `groupby` und `pivot_table`
    - **Explorative Analyse** systematisch durchf√ºhren
    - **Erkenntnisse** zusammenfassen und interpretieren

---

??? question "Selbstkontrolle"
    1. Warum verwendet man `errors='coerce'` bei der Typkonvertierung?
    2. Wie berechnet man die T√∂dlichkeitsrate pro Gruppe?
    3. Was bedeutet `dropna=False` bei `value_counts()`?
    4. Wann ist ein Datensatz "sauber genug" f√ºr die Analyse?
    
    ??? success "Antworten"
        1. Ung√ºltige Werte werden zu NaN statt einen Fehler zu werfen ‚Äì wichtig bei unstrukturierten Daten
        2. `.groupby('Gruppe')['Fatal'].mean() * 100` ‚Äì mean() auf True/False gibt den Anteil True
        3. NaN-Werte werden auch gez√§hlt statt ignoriert ‚Äì wichtig um fehlende Werte zu sehen
        4. Wenn die verbleibenden Probleme die Analyse nicht verf√§lschen und die Kernfragen beantwortet werden k√∂nnen ‚Äì Perfekte Daten gibt es selten!
