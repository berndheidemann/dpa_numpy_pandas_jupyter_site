# Pandas ‚Äì Transformation & Datenbereinigung

## Lernziele

Nach Bearbeitung dieses Arbeitsblatts kannst du:

- Spalten transformieren mit `map()`, `apply()`, und `applymap()`
- Daten bereinigen: fehlende Werte, Duplikate, Ausrei√üer
- neue Spalten berechnen und hinzuf√ºgen
- Datentypen konvertieren und kategorische Daten erstellen

!!! note "Begleitende Infobl√§tter"
    - [:material-book-open-variant: Pandas Transformation](../infoblaetter/pandas-transformation.md) ‚Äì map, apply, applymap
    - [:material-book-open-variant: Datenbereinigung](../infoblaetter/datenbereinigung.md) ‚Äì Cleaning Pipeline
    
    Lies die Infobl√§tter **zuerst**, bevor du die Aufgaben bearbeitest. Dort findest du alle Syntax-Beispiele und Erkl√§rungen.

---

## Einf√ºhrung

Echte Daten sind selten perfekt. Transformation und Bereinigung sind oft die zeitaufw√§ndigsten Schritte der Datenanalyse.

<div class="notebook-hint" markdown>
<span class="notebook-hint-icon">üìì</span>
<div class="notebook-hint-text">
<strong>Bearbeite alle Aufgaben in einem Jupyter Notebook</strong>
<small>√ñffne JupyterLite direkt im Browser ‚Äì keine Installation n√∂tig!</small>
</div>
<a href="../jupyter/lab/index.html" target="_blank" class="jupyter-button">
üöÄ JupyterLite √∂ffnen
</a>
</div>

**F√ºr dieses Arbeitsblatt verwendest du den MBA-Datensatz (`mba_decisions.csv`).**

!!! abstract "Datensatz herunterladen"
    [:material-download: mba_decisions.csv](../assets/files/mba_decisions.csv){ .md-button }

!!! info "Daten-Pipeline"
    **Rohdaten** ‚Üí **Bereinigung** (NaN, Duplikate, Ausrei√üer) ‚Üí **Transformation** (berechnen, umkodieren, konvertieren) ‚Üí **Saubere Daten**

---

## Aufgaben

### Aufgabe 1 ‚Äì Datensatz laden und Probleme identifizieren

Lade den MBA-Datensatz und verschaffe dir einen √úberblick √ºber Datenqualit√§tsprobleme.

- [ ] Lade den MBA-Datensatz (`mba_decisions.csv`) und gib die Shape aus
- [ ] Zeige die Datentypen aller Spalten an
- [ ] Finde heraus, welche Spalten **fehlende Werte** haben und wie viele (absolut und in Prozent)
- [ ] Pr√ºfe, ob der Datensatz **Duplikate** enth√§lt und zeige diese an

!!! tip "Hilfe"
    - Datensatz laden: `pd.read_csv(pfad)`
    - Datentypen und Info: `df.info()`, `df.dtypes`
    - Fehlende Werte: `df.isnull().sum()` f√ºr absolute Anzahl
    - Prozent berechnen: `(df.isnull().sum() / len(df) * 100)`
    - Duplikate z√§hlen: `df.duplicated().sum()`
    - Duplikate anzeigen: `df[df.duplicated(keep=False)]`

---

### Aufgabe 2 ‚Äì Fehlende Werte behandeln

Lerne verschiedene Strategien kennen, um mit fehlenden Werten umzugehen.

- [ ] Erstelle eine Kopie des DataFrames mit `df.copy()`
- [ ] **Strategie 1**: Entferne alle Zeilen mit fehlenden Werten ‚Äì wie viele Zeilen bleiben √ºbrig?
- [ ] **Strategie 2**: Entferne nur Zeilen, bei denen `GPA` oder `Work_Experience` fehlt
- [ ] **Strategie 3**: F√ºlle fehlende GPA-Werte mit dem **Mittelwert** der Spalte
- [ ] **Strategie 4**: F√ºlle fehlende `Work_Experience`-Werte mit dem **Median** (robuster gegen Ausrei√üer)

!!! tip "Hilfe"
    - Zeilen entfernen: `df.dropna()` oder `df.dropna(subset=['Spalte1', 'Spalte2'])`
    - Mit Mittelwert f√ºllen: `df['Spalte'].fillna(df['Spalte'].mean())`
    - Mit Median f√ºllen: `df['Spalte'].fillna(df['Spalte'].median())`
    - Mit festem Wert f√ºllen: `df['Spalte'].fillna(0)` oder `df['Spalte'].fillna('Unbekannt')`

!!! question "Reflexionsfrage"
    Wann ist der Median besser geeignet als der Mittelwert zum F√ºllen von L√ºcken?

---

### Aufgabe 3 ‚Äì Duplikate entfernen

Lerne, wie du Duplikate erkennst und kontrolliert entfernst.

- [ ] Erstelle eine Kopie des DataFrames
- [ ] Ermittle die Anzahl der Zeilen **vor** dem Entfernen von Duplikaten
- [ ] Entferne alle **vollst√§ndigen Duplikate** (alle Spalten identisch)
- [ ] Entferne Duplikate basierend nur auf den Spalten `Gender`, `GPA` und `Work_Experience`
- [ ] Experimentiere mit dem `keep`-Parameter: Was passiert bei `keep='first'`, `keep='last'` und `keep=False`?

!!! tip "Hilfe"
    - Duplikate entfernen: `df.drop_duplicates()`
    - Nur bestimmte Spalten pr√ºfen: `df.drop_duplicates(subset=['Spalte1', 'Spalte2'])`
    - Parameter `keep='first'` beh√§lt erstes Vorkommen, `keep='last'` das letzte, `keep=False` entfernt alle

---

### Aufgabe 4 ‚Äì Neue Spalten berechnen

Erstelle neue Spalten durch Berechnungen und bedingte Logik.

- [ ] Erstelle eine Spalte `GPA_Prozent`, die den GPA als Prozent von 4.0 ausdr√ºckt (gerundet auf 1 Dezimalstelle)
- [ ] Erstelle eine Spalte `High_GPA`, die "Ja" enth√§lt wenn GPA ‚â• 3.5, sonst "Nein"
- [ ] Erstelle eine Spalte `GPA_Rating` mit den Kategorien:
    - "Excellent" f√ºr GPA ‚â• 3.8
    - "Good" f√ºr GPA ‚â• 3.5
    - "Average" f√ºr GPA ‚â• 3.0
    - "Below Average" sonst
- [ ] Erstelle eine Spalte `Exp_Kategorie` mit `pd.cut()`, die Work_Experience in die Gruppen "Junior" (0-2), "Mid" (2-5), "Senior" (5-10) und "Expert" (>10) einteilt
- [ ] Erstelle einen gewichteten `Score` aus GPA (60%) und Work_Experience (10%)

!!! tip "Hilfe"
    - Direkte Berechnung: `df['neu'] = df['alt'] * 2`
    - Bedingt (2 Werte): `np.where(df['x'] >= 3.5, 'Ja', 'Nein')`
    - Bedingt (mehrere): `np.select([bed1, bed2, bed3], ['A', 'B', 'C'], default='D')`
    - Kategorien mit Grenzen: `pd.cut(df['Spalte'], bins=[0, 2, 5, 10, np.inf], labels=['A', 'B', 'C', 'D'])`

---

### Aufgabe 5 ‚Äì map() f√ºr Wertersetzung

Verwende `map()` um Werte systematisch zu ersetzen oder umzukodieren.

- [ ] Erstelle ein Dictionary, das die Decision-Werte ins Deutsche √ºbersetzt: Admit‚Üí"Aufgenommen", Deny‚Üí"Abgelehnt", Waitlist‚Üí"Warteliste"
- [ ] Wende das Dictionary mit `map()` an und speichere das Ergebnis in `Decision_DE`
- [ ] Schreibe eine Funktion `gpa_to_grade()`, die einen GPA-Wert in eine Buchstabennote umwandelt (A, B, C, D)
- [ ] Wende diese Funktion mit `map()` auf die GPA-Spalte an
- [ ] Achte darauf, dass die Funktion mit NaN-Werten umgehen kann

!!! tip "Hilfe"
    - Dictionary-Mapping: `df['neu'] = df['alt'].map({'wert1': 'ersatz1', 'wert2': 'ersatz2'})`
    - Funktion-Mapping: `df['neu'] = df['alt'].map(meine_funktion)`
    - NaN pr√ºfen: `if pd.isna(wert): return None`

!!! question "Eigenst√§ndige Transformations√ºbungen"
    L√∂se ohne Musterl√∂sung:
    
    1. **Mapping**: Erstelle eine Spalte `Decision_Code` die Admit=1, Waitlist=0, Deny=-1 zuordnet
    2. **Berechnung**: Erstelle einen "Composite Score" = (GPA * 10) + (Work_Experience * 2)
    3. **Kategorisierung**: Teile GPA in Quartile ein (Q1, Q2, Q3, Q4) mit `pd.qcut()`
    4. **String-Transformation**: Erstelle eine Spalte mit dem Format "[Gender] Bewerber mit [GPA] GPA"
    5. **Mehrfach-Bedingung**: Erstelle eine "Risk Score" Spalte:
        - 3 wenn GPA < 3.0 UND Experience < 2
        - 2 wenn GPA < 3.2 ODER Experience < 3
        - 1 wenn GPA < 3.5
        - 0 sonst

---

### Aufgabe 6 ‚Äì apply() f√ºr komplexe Transformationen

Verwende `apply()` f√ºr Transformationen, die mehrere Spalten oder komplexe Logik erfordern.

- [ ] Verwende `apply()` mit einer Lambda-Funktion, um alle GPA-Werte auf eine Dezimalstelle zu runden
- [ ] Schreibe eine Funktion `create_profile(row)`, die einen Profil-String erzeugt im Format: "Gender, GPA=X.X, Exp=Xy"
- [ ] Wende diese Funktion zeilenweise an (axis=1) und speichere das Ergebnis in einer neuen Spalte `Profile`
- [ ] Schreibe eine Funktion `predict_admission(row)`, die basierend auf GPA, Work_Experience und International einen Score berechnet und "Likely Admit", "Uncertain" oder "Unlikely" zur√ºckgibt
- [ ] Vergleiche deine Vorhersagen mit der tats√§chlichen Decision-Spalte (z.B. mit einer Kreuztabelle)

!!! tip "Hilfe"
    - apply auf Spalte: `df['Spalte'].apply(lambda x: x * 2)`
    - apply auf Zeilen: `df.apply(meine_funktion, axis=1)`
    - Auf Zeilenwerte zugreifen: `row['Spaltenname']`
    - Kreuztabelle: `pd.crosstab(df['Spalte1'], df['Spalte2'])`

---

### Aufgabe 7 ‚Äì Datentypen konvertieren

Konvertiere Datentypen f√ºr effizientere Speicherung und korrekte Analysen.

- [ ] Zeige die aktuellen Datentypen aller Spalten an
- [ ] Konvertiere die Spalte `Decision` in den Typ `category`
- [ ] Vergleiche den Speicherverbrauch vor und nach der Konvertierung
- [ ] Erstelle eine **ordinale Kategorie** f√ºr Work_Experience mit den Stufen "Low", "Medium", "High" (geordnet!)
- [ ] √úberpr√ºfe, ob die Kategorie tats√§chlich geordnet ist

!!! tip "Hilfe"
    - Zu Kategorie: `df['Spalte'].astype('category')`
    - Speicherverbrauch: `df['Spalte'].memory_usage()`
    - Ordinale Kategorie: `pd.Categorical(werte, categories=['A', 'B', 'C'], ordered=True)`
    - Ist geordnet? `df['Spalte'].cat.ordered`

---

## Vertiefende Aufgaben

!!! tip "Optionale Aufgaben zur Vertiefung"
    Die folgenden Aufgaben sind **optional** und vertiefen das Gelernte. Sie eignen sich besonders f√ºr:
    
    - **String-Bereinigung mit str-Methoden (strip, lower, replace)**
    - **Ausrei√üer-Behandlung mit der IQR-Methode**
    - **Pr√ºfungsvorbereitung** durch eigenst√§ndiges Arbeiten

---

### Aufgabe 8 ‚Äì Strings bereinigen

Bereinige Text-Daten f√ºr konsistente Analysen.

- [ ] Entferne f√ºhrende und nachfolgende Leerzeichen aus allen String-Spalten
- [ ] Erstelle Varianten der `Gender`-Spalte: Kleinbuchstaben, Gro√übuchstaben, Title Case
- [ ] Ersetze in der `Major`-Spalte das Zeichen "&" durch "and"
- [ ] √úberlege: Welche weiteren String-Bereinigungen k√∂nnten bei echten Daten n√∂tig sein?

!!! tip "Hilfe"
    - String-Spalten finden: `df.select_dtypes(include=['object']).columns`
    - Whitespace entfernen: `df['Spalte'].str.strip()`
    - Kleinbuchstaben: `df['Spalte'].str.lower()`
    - Ersetzen: `df['Spalte'].str.replace('alt', 'neu', regex=False)`

---

### Aufgabe 9 ‚Äì Ausrei√üer behandeln

Identifiziere und behandle Ausrei√üer mit verschiedenen Methoden.

- [ ] Berechne f√ºr die Spalte `GPA` die Quartile Q1 und Q3 sowie den Interquartilsabstand (IQR)
- [ ] Bestimme die Ausrei√üer-Grenzen nach der IQR-Methode (Q1 - 1.5√óIQR und Q3 + 1.5√óIQR)
- [ ] Finde alle Ausrei√üer und gib deren Anzahl und Werte aus
- [ ] Erstelle eine neue Spalte `GPA_clipped`, in der Ausrei√üer auf die Grenzen begrenzt werden (Capping)
- [ ] **Bonus**: Berechne f√ºr `Work_Experience` den Z-Score und finde Werte mit |Z| > 3

!!! tip "Hilfe"
    - Quartile: `df['Spalte'].quantile(0.25)` f√ºr Q1
    - Filter f√ºr Ausrei√üer: `df[(df['Spalte'] < lower) | (df['Spalte'] > upper)]`
    - Capping/Clipping: `df['Spalte'].clip(lower=grenze_unten, upper=grenze_oben)`
    - Z-Score: `(df['Spalte'] - df['Spalte'].mean()) / df['Spalte'].std()`

---

### Aufgabe 10 ‚Äì Komplette Bereinigungspipeline

Kombiniere alle gelernten Techniken zu einer wiederverwendbaren Bereinigungsfunktion.

- [ ] Schreibe eine Funktion `clean_mba_data(df)`, die folgende Schritte durchf√ºhrt:
    1. Kopie des DataFrames erstellen
    2. Strings in allen Textspalten trimmen
    3. Fehlende Werte in `GPA` und `Work_Experience` mit dem Median f√ºllen
    4. Duplikate entfernen
    5. GPA-Ausrei√üer mit Capping behandeln
    6. Eine neue Spalte `GPA_Category` mit den Stufen "Low", "Medium", "High" hinzuf√ºgen
    7. Kategorische Spalten (`Gender`, `International`, `Major`, `Decision`) in den Typ `category` konvertieren
- [ ] Wende die Funktion auf den Original-Datensatz an
- [ ] Validiere das Ergebnis: Shape, Datentypen, fehlende Werte

!!! tip "Hilfe"
    - Funktion definieren: `def clean_mba_data(df): ...`
    - Am Ende: `return df`
    - Validierung: `df.info()`, `df.isnull().sum()`, `df.dtypes`

---

### Aufgabe 11 ‚Äì Komplexe Transformationsaufgaben

!!! warning "Ohne Musterl√∂sung"
    Diese Aufgaben erfordern Kombination mehrerer Transformationstechniken.

**Aufgabe A: Feature Engineering**

- [ ] Erstelle mindestens 5 neue, sinnvolle Features aus den bestehenden Daten:
    - Ein bin√§res Feature (ja/nein)
    - Ein numerisches abgeleitetes Feature
    - Ein kategorisches Feature mit mehr als 2 Kategorien
    - Ein Interaktions-Feature (Kombination aus 2 Spalten)
    - Ein Ranking-Feature (z.B. GPA-Rang in Prozent)

**Aufgabe B: Datenbereinigung ohne Vorgabe**

- [ ] Implementiere eine vollst√§ndige Bereinigungsfunktion f√ºr den MBA-Datensatz:
    1. Identifiziere alle Probleme (NaN, Duplikate, Inkonsistenzen)
    2. Dokumentiere, wie viele Datens√§tze jedes Problem haben
    3. Behebe die Probleme mit geeigneten Strategien
    4. Validiere, dass die Bereinigung erfolgreich war

**Aufgabe C: Transformation mit apply()**

- [ ] Schreibe eine `apply()`-Funktion, die f√ºr jeden Bewerber eine "Prediction" macht:
    - Analysiere erst: Was unterscheidet Aufgenommene von Abgelehnten?
    - Entwickle eine Logik mit mindestens 4 Kriterien
    - Wende sie auf alle Zeilen an
    - Berechne die Genauigkeit (% korrekte Vorhersagen)

**Aufgabe D: String-Manipulation**

Wenn der Datensatz Text-Spalten enth√§lt:

- [ ] Bereinige alle Strings (Leerzeichen, Gro√ü-/Kleinschreibung)
- [ ] Extrahiere relevante Informationen aus Text
- [ ] Erstelle Dummy-Variablen aus kategorischen Text-Spalten

**Aufgabe E: Pipeline erstellen**

- [ ] Erstelle eine wiederverwendbare Funktion `prepare_mba_data(filepath)`:
    - L√§dt die Daten
    - Bereinigt sie
    - F√ºgt alle neuen Features hinzu
    - Konvertiert Datentypen
    - Gibt einen sauberen DataFrame zur√ºck
- [ ] Dokumentiere jeden Schritt mit Kommentaren.

---

## Zusammenfassung

!!! success "Das hast du gelernt"
    - **Fehlende Werte**: `dropna()`, `fillna()`, `ffill()`, `bfill()`
    - **Duplikate**: `duplicated()`, `drop_duplicates()`
    - **map()**: Wertersetzung mit Dictionary oder Funktion
    - **apply()**: Komplexe Transformationen (axis=0/1)
    - **Neue Spalten**: Direkte Berechnung, `np.where()`, `np.select()`
    - **Datentypen**: `astype()`, `pd.to_numeric()`, `pd.Categorical()`
    - **Ausrei√üer**: IQR-Methode, Z-Score, `clip()`

---

??? question "Selbstkontrolle"
    1. Wann verwendest du `map()` vs. `apply()`?
    2. Wie f√ºllst du NaN mit dem Median einer Spalte?
    3. Was macht `df.clip(lower=0, upper=100)`?
    4. Wie erstellst du eine ordinale (geordnete) Kategorie?
    
    ??? success "Antworten"
        1. `map()` f√ºr einfache 1:1 Ersetzungen (Dictionary, Funktion auf einzelne Werte); `apply()` f√ºr komplexere Logik oder wenn mehrere Spalten n√∂tig sind (axis=1)
        2. `df['col'] = df['col'].fillna(df['col'].median())`
        3. Begrenzt alle Werte auf den Bereich 0-100 (Capping)
        4. `pd.Categorical(values, categories=['A', 'B', 'C'], ordered=True)`
